Simple Storage Service (hence the name S3)
(12 question from this category will be asked at the least)


S3-Optimization -http://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html
Amazon S3 automatically scales to high request rates. 
For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. 
There are no limits to the number of prefixes in a bucket. 

CORS in S3.

How to name a file for optimum performance? (Also, know the best filename format in S3 to optimize performance.)

Know/review limits for S3
1. 100 Buckets per account
2. After you have created a bucket, you can't change its Region
3. No Buckets within a bucket.
4. 


S3 provides developers and IT teams with secure, durable, highly scalable object storage.

It is not for OS or DBMS.

The data is spread across multiple devices and facilities.

There is unlimited storage.

Bucket ~ Folder

It is object based storage.

S3 is a universal namesapce. That is, names must be unique globally.
e.g https://s3-eu-west-1.amazonaws.com/acloudguru

2- Data Model Consistency

Read after Write Consistency- as soon as you upload it is available.

Eventual Consistency for overwrite PUTS and DELETES.

S3 is a simple key, value store.
S3 supports version control.

S3 Object Anatomy {
    key
    value
    version ID
    MetaData
    Subresources -  bucket specific configuration. (CORS and Bucket Policies)
}


Built for 99.99% availability for s3 platform
Amazon Guarentees 99.9% availability
Amazon Guarentees 99.999999999 (11 9's) of durability.
It is designed to sustain the loss of 2 facilities concurrently.


S3- Storage Tiers/Classes [check image]
1. s3 (Standard)
2. S3-IA (Infrequently Accessed): For data that is accessed less frequently,
   but requires rapid access when needed.
3. S3- One Zone IA: Same as IA however data is stored in a single availability Zone only,
  still 99.999999999 durability, but only 99.5% availability.
  Cost is 20% less than regular S3-IA
4. Reduced Redundancy Storage: Designed to provide 99.99% durability and 99.99% availability of objects over a given year.
Used for data that can be recreated if lost, e.g thumbnails. (Starting to disappear from AWS documentation but may still feature in exam)
5. Glacier: Very Cheap, but used for archival only. Optimised for data that is infrequently accessed.


Lifecycle Management
Versioning
Encryption

They key name decides in which partition the file is stored in.
Charges
1. Storage per GB
2. Requests (GET PUT etc)
3. Storage Management pricing. Inventory, analytics etc
4. Data Managment pricing i.e downloading file (data transfeered out of s3)
5. Transfer Acceleration (Use of cloudfront to optimize transfers)

Files can be from 0 Bytes to 5 TB.

S3-Security

By Default all buckets are PRIVATE.

You can set up access control to your buckets using:
 Bucket Policies - Applied at bucket level.
 Access Control Lists - Applied at Object Level.
S3 Buckets can be configured to create access logs, which log all requests made to the s3 bucket.
These logs can be written to another bucket.
Bucket Policy are written in JSON.


LAB (S3 Policy etc)

2 encrypiton is offerred AES-256 and AWS-KMS

1.In-Transit:
SSL/TLS (HTTPS)

2.At-REST
Server Side Encryption:
 S3 Managed Keys - SSE-S3
 AWS Key Management Service, Managed Keys, SSE-KMS
 Server Side Encryption with customer provided keys - SSE-C

3.Client Side Encryption

If you want to enforce the use of encryption for oyur files stored in S3, use an S3 Bucket Policy
to deny all PUT requests that don't include the x-amz-server-side-encryption parameter in the request header.

Every time a file is uploaded a PUT request is initiated.

In AWS key names determine which partition the object(file) is stored in - you could add a hax prefix to file name for better performance.

GET-Intensive Workloads : Use CloudFront

Mixed Workloads (GET, PUT & DELETE) : Use hax prefix to S3 object key names to prevent multiple objects being stored on the same partition.


CORS - Configuration LAB (pending)[do it. Made mistakes in the question]

Cross Region Replication
Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different AWS Regions. 

Configuration Required>>>
The destination bucket, where you want Amazon S3 to replicate objects
An AWS IAM role that Amazon S3 can assume to replicate objects on your behalf

Criterias for CRR:
Both source and destination buckets must have versioning enabled.

The source and destination buckets must be in different AWS Regions.

Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket on your behalf.

If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner READ and READ_ACP permissions with the object ACL


CloudFront

CDN: A content delivery network is a system of distributed servers that
Basically helps you reduce network latency and hops.

Edge Locations used by Cloud Front to keep copies of cache file which is much closer to the geographihc location.

origin: This is the origin of all the files the CDN will distribute, which can be an s3 bucket, an ec2 instance an elastic load balancer,route53 or elastic load balancer.

Distribution is a collection of edge locations
1. Web Distribution - Typically used for websites.HTTP, HTTPS
2. RTMP- Protocol used for Media Streaming


What is CloudFront?

Amazon CloudFront can be used to deliver your entire website, including dynamic, static, streaming, and interactive content using a  global network of edge locations. Requests for your content are automatically routed to the nearest edge locations, so the content is delivered with the best possible performance.

e.g Can be used to be used to optimize performance for users accessing a website backed by s3.

CloudFront works with s3, route53, ec2 and also with non aws server

Objects are cached for TTL value. The Time to Live value expires, cached object is deleted.

You can clear cached objects but you will be charged.


Create Distribution:

1. Improve your loading speed by creating a cloudfront and serve some images nigga
2.
3.

S3 performance update

3500 put operations
5500 get operations

What is hex hash prefix in s3?

At a time you can upload a max of 5GB. Use multipart upload if the file size is greater than that bruh.



////////////////gotta format and shit
S3 FAQ Notes

1. S3 Standard and Standard IA ,Glacier storage provides the 99.999999999 durability (11 nines )

2. Also provide Access control ,Versioning ,CORS and backup etc.

3. When storing the objects in s3 it store in different devices and in different facilities before returning success to upload.

4. Checksum using MD5 and cycle redundancy check (CRSâ€™s) to detect the data corruption .Amazon performs these checks on data at rest fixed using redundant data to maintain integrity of data.

5. Versioning is extra layer of durability and you can preservee ,retrieve and retain any version of data

6. By default it returns the latest copy ,to get the overwritten or deleted data you can mention version and retrieve it .

7. When user deletes the objects it will not be available for subsequent request (unversioned) but previous version will be preserved.

8. User can create the rollback window using lifecycle of storing into glacier and delete after 100 days so rollback window is 100 days by saving storage costs also.

9. Versioning MFA delete provide maximum protection of my preserve data. So was account credential and MFA token to delete anything

10. S3 Standard IA class is for Infrequent access data with high speed and low latency and wit low code of receive and storage per Gb.

11. S3 standard IA perform same performance as compared to S3 Standard

12. Standard IA provide 99.9% avialablity.

13. You can directly add the objects by specifying the STANDARD_IA in X-Ams-Storage-Class header or use the lifecycle policy to mover from standard to Standard IA

14. latancy and through will be same standard class

15. Although it is for long lived data but data should be there standard IA for minimum 30days otherwise it will be charged for full 30 days.min limit is 30 days

16. Minimum size for Standard IA is 128kb ,whatever be the size less than 128 kb you have to pay for 128 kb

17. Glacier is even more cheaper than others and used for archival data and cab be accessed slowly might takes minutes to hours

18. Set the lifecycle rules to move data to glacier using prefix and period like /logs and 180days of obj creation

19. Initiate retreival request to retrieve the data stored in glacier through console or API ,while retrieving AWS copy the glacier data into RRS and provide temporary copy and you can mention the time for which you want that data to be in RRS.

20. Three option for Glacier Retrieval : Expedited(1-5min) , standard (3-5 hours) ,Bulk(5-12 hours)

21. $0.004 per gb/mont for glacier storage and transition to Glacier ($0.05 /1000request) and minumium 90 days for Glacier.

22. 10 Gb retrieval is free

23. If you delete 1 gb of data from glacier in 1st month out of 3 then you have to pay remaining 2 month of glacier storage price for 1 gb

24. Retrieval cost from glacier is 0.03 per gb and 0.01 per request,0.01 gb and 0.05 per 1000 requet ,0.0025 per Gb and 0.025 per 1000 request

25. Yes you can host static websites on s3 on low cost ,highly available,scable solution

26. Yes you can Tap your domain name to you bucket

27. You can redirect to other url or using bookmarks to old page or using policy

28. No additional charge only storage ,request and data transfer

29. Objects Tags (key values ) can be used for IAM and lifecycle policies and customize metric storage

30. You can add upto 10 tags to s3 object while creating or later

31. Outside AWS console all the tags need to add in set example if you have added 5 tags and want to add 6th ,you have to mention all five

32. $0.01 per 10000 tags per month

33. Storage Analytics or Storage class analysis automatically define the patterns and identify the right storage class foe right data . Later you can create lifecycle policy using that data

34. Use S3 put bucket analysis API to configure a storage class analysis .you can navigate to AWS Management console ,management tab to manage s3 analytics ,s3 inventory,s3 cloud metrics

35. S3 analytics happen daily an you can export analysis data on your bucket for further checks

36. S3 inventory is alternative for S3 sync list Api which will provide the .csv or ORC file containing the output of your obj and metadata

37. You can use s3 put bucket api to configure the daily and weekly inventory for all the obj or prefix and mention the destination bucket where .csv or ORC and metadata like size ,modified at a etc will be stored

38. You can encrypt inventory using SSE -S3 ,SSE -KMS

39. You can s3 inventory as direct input to your app or also query

40. You can set lifecycle policy to reduce cost the cost of storage ,it mover obj from one class to another .Also delete the obj if it is not required depends upon the days you give. Also Delete the multiple part expiration files

41. Life cycle object applies to existing objects and new object which will come

42. Cross region replication is to replicate object ,there ACL and metadata to destination bucket in the region. To provide low latency data access in the different region

43. Versioning must be on while using CORS

44. CORS is bucket level configuration you can enable by giving destination bucket

45. Use s3 copy for previous data before CORS

46. You can mention the subset to replication suing prefix

47. Yes you can replicate the encrypted data by mentioning the Key in the configuration

48. Transfer Accelararion can help in transfer data fast for long distance using cloud front edge location

49. Nickname.s3-accelarate.awsamozon.com or backutname.s3-accrete.dulstack.awaamaazon.com

50. Fast depends on bandwidth ,packet loss and distance

51. Accelreate is secure are Over TCP and can restrict IP

52. S3 acceleration is giving throuput and TCP it is better than cloud front if you have data < 5 gb then use put/post commands

53. Snowball is idea for customer transferring batch at once and 5-7 days turnaround.Tranfer acceleration can use 1 gb bandwidth and upload 75 TB of Data if possible than this is better.

54. Other option is perform initial heavy lift using snowball and use transfer acc later

55. AWS Direct Connect is for customer having private network ,Tranfer acc is best for all from diff loc over public internet where variable network condition make throughput poor. Some Aws direct customer use transfer acc where internet speed is slow

56. Gateways that connect directly to s3 can tae advantages of transfer acc

57. S3 support ipv6 ,use dual stack endpoints which supports both 6 and 4

58. http://s3.dualstak.region.awsamozon.com/bucketname and bucket.s3.dualstack.region.awsamaomzon.com


Q) Question will be asked about Performance.
Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. It is simple to increase your read or write performance exponentially. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.

Q) What is TCP Window Scaling
TCP window scaling allows you to improve network throughput performance between your operating system and application layer and Amazon S3 by supporting window sizes larger than 64 KB. At the start of the TCP session, a client advertises its supported receive window WSCALE factor, and Amazon S3 responds with its supported receive window WSCALE factor for the upstream dire.

Q) TCP Selective Acknowledgement
TCP selective acknowledgement is designed to improve recovery time after a large number of packet losses. TCP selective acknowledgement is supported by most newer operating systems, but might have to be enabled.


Q) Host static websites on S3

You can host a static website on Amazon Simple Storage Service (Amazon S3). On a static website, individual webpages include static content. They might also contain client-side scripts. No server side scripts though.
<bucket-name>.s3-website-<AWS-region>.amazonaws.com
Q:     How much data can I store in Amazon S3?

The total volume of data and number of objects you can store are unlimited. 
Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. 
The largest object that can be uploaded in a single PUT is 5 gigabytes. 
For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.


Q:  How can I control access to my data stored on Amazon S3?

Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication. IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do. With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. 
Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address. 
With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object. 
With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time.

Q:  What is an Amazon VPC Endpoint for Amazon S3?

An Amazon VPC Endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to S3. 
The VPC Endpoint routes requests to S3 and routes responses back to the VPC. For more information about VPC Endpoints, read.

If you want to do Replication at bucket level, do cross region replication(aka CRR).


Q:  What checksums does Amazon S3 employ to detect data corruption?

Amazon S3 uses a combination of Content-MD5 checksums and cyclic redundancy checks (CRCs) to detect data corruption

Q) Versioning

Q:  What is S3 Intelligent-Tiering?

Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is an S3 storage class for data with unknown access patterns or changing access patterns that are difficult to learn. It is the first cloud storage class that delivers automatic cost savings by moving objects between two access tiers when access patterns change. One tier is optimized for frequent access and the other lower-cost tier is designed for infrequent access.

Use AWS Lifecycle rules to PUT to data or use the API to move data to Glacier


Object Lifecylce Mgmt.

Transaction and Expiration actions.

Q) What is "Query in Place" functionality?

Amazon S3 allows customers to run sophisticated queries against data stored without the need to move data into a separate analytics platform. 
The ability to query this data in place on Amazon S3 can significantly increase performance and reduce cost for analytics solutions leveraging S3 as a data lake. 

Q:  What is Amazon Athena?

Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately.

Q) How do you speed up S3 download?
Use Amazon S3 Transfer Accelararion. 
Your data transfer application must use one of the following two types of endpoints to access the bucket for faster data transfer: .s3-accelerate.amazonaws.com or .s3-accelerate.dualstack.amazonaws.com 
It also supports Multipart Upload.

Q:  What is S3 Inventory?

The S3 Inventory report provides a scheduled alternative to Amazon S3â€™s synchronous List API. You can configure S3 Inventory to provide a CSV, ORC, or Parquet file output of your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix.


Website Hosting Required permissions?
1. Make Bucket Public.
2. Enable static web hosting in your bucket
3. Add an index.js file

The bucket name can be between 3 and 63 characters long, and can contain only lower-case characters, numbers, periods, and dashes.


ERROR CODE for s3?
403 [AccessDenied] Access Related 
400 [TooSmall, TooLarge shit like that] BAD REQUEST
409 [bucketAlreadyExists, ] CONFLICT

When do you get this error?
You are working with the S3 API and receive an error message: 409 Conflict. What is the possible cause of this error? Choose the correct answer from the options below

You can remove the bucket only after removing the contents, otherwise face 409 error

Any objects uploaded prior to versioning will have the version ID as NULL.

Amazon Athena:
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.


