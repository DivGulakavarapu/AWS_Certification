AWS Simple Storage Service (S3)

## THEORY ##

- S3 provides peeps with secure, durable, highly scalable object storage.
- It is not for OS or DBMS.
- The data is spread across multiple devices and facilities.
- There is unlimited storage.
- Bucket ~ Folder
- It is object based storage. S3 is a simple key, value store.
- S3 supports version control.
- S3 is a universal namespace. That is, names must be unique globally.
e.g https://s3-eu-west-1.amazonaws.com/somename

S3 Object Anatomy {
    key
    value
    version ID
    MetaData
    Subresources -  bucket specific configuration. (CORS and Bucket Policies)
}

DURABILTY AND AVAILABILITY
- Built for 99.99% availability for s3 platform.
- Amazon Guarentees 99.9% availability
- Amazon Guarentees 99.999999999 (11 9's) of durability.
- It is designed to sustain the loss of 2 facilities concurrently.

LIMITS FOR S3
1. 100 Buckets per account
2. After you have created a bucket, you can't change its Region.
3. No Buckets within a bucket.
4. Files can be from 0 Bytes to 5 TB.
5. Your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. 
6. There are no limits to the number of prefixes in a bucket. 

S3- STORAGE TIERS/CLASSES [check image]
1. S3 (Standard)
2. S3-IA (Infrequently Accessed): For data that is accessed less frequently,
   but requires rapid access when needed.
3. S3- One Zone IA: Same as IA however data is stored in a single availability Zone only, still 99.999999999 durability, but only 99.5% availability.
   Cost is 20% less than regular S3-IA
4. Reduced Redundancy Storage: Designed to provide 99.99% durability and 99.99% availability of objects over a given year.
Used for data that can be recreated if lost, e.g thumbnails. (Starting to disappear from AWS documentation but may still feature in exam)
5. Glacier: Very Cheap, but used for archival only. Optimised for data that is infrequently accessed. 

How to name a file for optimum performance? (Also, know the best filename format in S3 to optimize performance.)
In AWS key names determine which partition the object(file) is stored in - you could add a hex prefix to file name for better performance.

DATA MODEL CONSISTENCY
- Read after Write Consistency: as soon as you upload it is available.
- Eventual Consistency for overwrite PUTS and DELETES.
- Checksum using MD5 and cycle redundancy check (CRS’s) to detect the data corruption.
- Amazon performs these checks on data at rest fixed using redundant data to maintain integrity of data.

CHARGES
1. Storage per GB
2. Requests (GET PUT etc)
3. Storage Management pricing. Inventory, analytics etc
4. Data Managment pricing i.e downloading file (data transfeered out of s3)
5. Transfer Acceleration (Use of cloudfront to optimize transfers)

S3 SECURITY
- By Default all buckets are PRIVATE.
- You can set up access control to your buckets using:
  1. Bucket Policies - Applied at bucket level.
  2. Access Control Lists - Applied at Object Level.
- S3 Buckets can be configured to create access logs, which log all requests made to the s3 bucket.
- These logs can be written to another bucket.
- Bucket Policy are written in JSON.
- 2 encrypiton is offerred AES-256 and AWS-KMS
  Encryption
    1. In-Transit:
    SSL/TLS (HTTPS)
    2. At-REST
    Server Side Encryption:
    - S3 Managed Keys - SSE-S3
    - AWS Key Management Service, Managed Keys, SSE-KMS
    - Server Side Encryption with customer provided keys - SSE-C
    3. Client Side Encryption
    - Client-side encryption is the act of encrypting data before sending it to Amazon S3.
    - To enable client-side encryption, you have the following options:
          3.1 Use an AWS KMS-managed customer master key
          3.2 Use a client-side master key
- If you want to enforce the use of encryption for your files stored in S3, use an S3 Bucket Policy.
- Deny all PUT requests that don't include the x-amz-server-side-encryption parameter in the request header.
- Every time a file is uploaded a PUT request is initiated.

CROSS REGION REPLICATION
- Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different AWS Regions. 
- Configuration is required for this.
- The destination bucket, where you want Amazon S3 to replicate objects.
- An AWS IAM role that Amazon S3 can assume to replicate objects on your behalf.
- Criterias for CRR:
  Both source and destination buckets must have versioning enabled.
  The source and destination buckets must be in different AWS Regions.
  Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket on your behalf.
  If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner READ and READ_ACP permissions with the object ACL.
- To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information.
- Versioning must be on while using CORS.

CLOUDFRONT
- Amazon CloudFront can be used to deliver your entire website, including dynamic, static, streaming, and interactive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge locations, so the content is delivered with the best possible performance.
- CloudFront is a CDN.
- A content delivery network is a system of distributed servers that basically helps you reduce network latency and hops.
- Edge Locations used by Cloud Front to keep copies of cache file which is much closer to the geographihc location.
- Origin: This is the origin of all the files the CDN will distribute, which can be an s3 bucket, an ec2 instance an elastic load balancer, route53 or elastic load balancer.
- Distribution is a collection of edge locations.
  1. Web Distribution - Typically used for websites.HTTP, HTTPS
  2. RTMP- Protocol used for Media Streaming
- Can be used to be used to optimize performance for users accessing a website backed by s3.
- GET-Intensive Workloads : Use CloudFront
- Mixed Workloads (GET, PUT & DELETE) : Use hex prefix to S3 object key names to prevent multiple objects being stored on the same partition.
- CloudFront works with s3, route53, ec2 and also with non aws server
- Objects are cached for TTL value. The Time to Live value expires, cached object is deleted.
- You can clear cached objects but you will be charged.
- For web distributions, to control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin,  you can configure your origin to add a Cache-Control or an Expires header field to each object.
- Specify a value for Minimum TTL in CloudFront cache behaviors.·Use the default value of 24 hours.

S3 VERSIONING
- Versioning is extra layer of durability and you can preservee, retrieve and retain any version of data.
- Buckets can be in one of three states: unversioned (the default), versioning-enabled, or versioning-suspended.
- Objects stored in your bucket before you set the versioning state have a version ID of null.
- The bucket owner (or any user with appropriate permissions) can suspend versioning to stop accruing object versions. 
- By default it returns the latest copy, to get the overwritten or deleted data you can mention version and retrieve it.
- When user deletes the objects it will not be available for subsequent request (unversioned) but previous version will be preserved.
- Any objects uploaded prior to versioning will have the version ID as NULL.

S3 OBJECT LIFECYCLE MANAGEMENT
- To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their lifecycle. A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects
- There are two actions.
  1. Transition Action - Object moves from one storage class to other.
  2. Expiration Action - Object is expired and then deleted.
- User can create the rollback window using lifecycle of storing into glacier and delete after 100 days so rollback window is 100 days by saving storage costs also.
- Lifecycle configuration is an XML file.

S3 TAGS
- Objects Tags (key values ) can be used for IAM and lifecycle policies and customize metric storage.
- You can add upto 10 tags to s3 object while creating or later.
- Outside AWS console all the tags need to add in set example if you have added 5 tags and want to add 6th, you have to mention all five.
- $0.01 per 10000 tags per month

S3 TRANSFER ACCELERATION
- Transfer Accelararion can help in transfer data fast for long distance using cloud front edge location.
- Nickname.s3-accelarate.awsamozon.com or backutname.s3-accrete.dulstack.awaamaazon.com
- Fast depends on bandwidth, packet loss and distance.
- Accelreate is secure are Over TCP and can restrict IP.
- S3 acceleration is giving throuput and TCP it is better than cloud front if you have data < 5 gb then use put/post commands
- Snowball is idea for customer transferring batch at once and 5-7 days turnaround. Tranfer acceleration can use 1 gb bandwidth and upload 75 TB of Data if possible than this is better
- Other option is perform initial heavy lift using snowball and use transfer acc later.
- AWS Direct Connect is for customer having private network, Tranfer acc is best for all from diff loc over public internet where variable network condition make throughput poor. Some Aws direct customer use transfer acc where internet speed is slow
- Gateways that connect directly to s3 can take advantages of transfer acceleration.

S3 LIMITS BRUH
- 3500 put operations per prefix in a bucket.
- 5500 get operations per prefix in a bucket.
- At a time you can upload a max of 5GB. Use multipart upload if the file size is greater than that bruh.
- S3 support ipv6, use dual stack endpoints which supports both 6 and 4.
- http://s3.dualstak.region.awsamozon.com/bucketname and bucket.s3.dualstack.region.awsamaomzon.com
- The bucket name can be between 3 and 63 characters long, and can contain only lower-case characters, numbers, periods, and dashes.

## QUESTIONS ##
Q) Question will be asked about Performance.
Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. 
There are no limits to the number of prefixes in a bucket. It is simple to increase your read or write performance exponentially. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.

Q) What is TCP Window Scaling?
TCP window scaling allows you to improve network throughput performance between your operating system and application layer and Amazon S3 by supporting window sizes larger than 64 KB. At the start of the TCP session, a client advertises its supported receive window WSCALE factor, and Amazon S3 responds with its supported receive window WSCALE factor for the upstream dire.

Q) TCP Selective Acknowledgement.
TCP selective acknowledgement is designed to improve recovery time after a large number of packet losses. 
TCP selective acknowledgement is supported by most newer operating systems, but might have to be enabled.

Q) Host static websites on S3.
You can host a static website on Amazon Simple Storage Service (Amazon S3).
On a static website, individual webpages include static content. They might also contain client-side scripts. No server side scripts though.
<bucket-name>.s3-website-<AWS-region>.amazonaws.com

Q) How much data can I store in Amazon S3?
The total volume of data and number of objects you can store are unlimited. 
Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. 
The largest object that can be uploaded in a single PUT is 5 gigabytes. 
For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.

Q) How can I control access to my data stored on Amazon S3?
Customers may use four mechanisms for controlling access to Amazon S3 resources
1. Identity and Access Management (IAM) policies.
2. Bucket Policies.
3. Access Control Lists (ACLs).
4. Query String Authentication.
IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account.
With IAM policies, customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do. With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. 
Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address. 
With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object. 
With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time.

Q) What is an Amazon VPC Endpoint for Amazon S3?
An Amazon VPC Endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to S3. 
The VPC Endpoint routes requests to S3 and routes responses back to the VPC.

Q) How would you replicate bucket?
If you want to do Replication at bucket level, do cross region replication(aka CRR).

Q) What checksums does Amazon S3 employ to detect data corruption?
Amazon S3 uses a combination of Content-MD5 checksums and cyclic redundancy checks (CRCs) to detect data corruption

Q) What is S3 Intelligent-Tiering?
Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is an S3 storage class for data with unknown access patterns or changing access patterns that are difficult to learn. It is the first cloud storage class that delivers automatic cost savings by moving objects between two access tiers when access patterns change. One tier is optimized for frequent access and the other lower-cost tier is designed for infrequent access.
Use AWS Lifecycle rules to PUT to data or use the API to move data to Glacier

Q) What is "Query in Place" functionality?
Amazon S3 allows customers to run sophisticated queries against data stored without the need to move data into a separate analytics platform. 
The ability to query this data in place on Amazon S3 can significantly increase performance and reduce cost for analytics solutions leveraging S3 as a data lake. 

Q) What is Amazon Athena?
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately.

Q) How do you speed up S3 download?
Use Amazon S3 Transfer Accelararion. 
Your data transfer application must use one of the following two types of endpoints to access the bucket for faster data transfer: .s3-accelerate.amazonaws.com or .s3-accelerate.dualstack.amazonaws.com 
It also supports Multipart Upload.

Q) What is S3 Inventory?
The S3 Inventory report provides a scheduled alternative to Amazon S3’s synchronous List API. You can configure S3 Inventory to provide a CSV, ORC, or Parquet file output of your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix.
S3 inventory is alternative for S3 sync list Api which will provide the .csv or ORC file containing the output of your obj and metadata.

Q) Website Hosting Required permissions?
1. Make Bucket Public.
2. Enable static web hosting in your bucket.
3. Add an index.js file.

Q) ERROR CODE for S3?
403 [AccessDenied] Access Related 
400 [TooSmall, TooLarge shit like that] BAD REQUEST
409 [bucketAlreadyExists, ] CONFLICT

Q) You are working with the S3 API and receive an error message: 409 Conflict. What is the possible cause of this error?
You can remove the bucket only after removing the contents, otherwise face 409 error

Q) TroubleShooting S3
Significant Increases in HTTP 503 Responses to Amazon S3 Requests to Buckets with Versioning Enabled.
If you notice a significant increase in the number of HTTP 503-slow down responses received for Amazon S3 PUT or DELETE object requests to a bucket that has versioning enabled, you might have one or more objects in the bucket for which there are millions of versions. When you have objects with millions of versions, Amazon S3 automatically throttles requests to the bucket to protect the customer from an excessive amount of request traffic, which could potentially impede other requests made to the same bucket.

Q) What does S3 copy command do?
Use this to transfer large segments of data from S3 to Redshift or DynamoDB.


